# -*- coding: utf-8 -*-
"""Summarizer_script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FSCxVMqo8iZxOK3Y-g6c1V-Wusv_AP2g
"""

!pip install transformers torch PyMuPDF

import fitz  # PyMuPDF
from transformers import pipeline

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    document = fitz.open(pdf_path)
    text = ''
    for page in document:
        text += page.get_text()
    document.close()
    return text

# Function to chunk text into manageable parts
def chunk_text(text, chunk_size=1024):
    # Split the text by whitespace to get words
    words = text.split()
    # Initialize chunks
    chunks = []
    current_chunk = []

    # Create chunks of words based on the approximate character length
    for word in words:
        current_chunk.append(word)
        if sum(len(w) + 1 for w in current_chunk) >= chunk_size:  # +1 for space
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Add the last chunk if any
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

from transformers import pipeline

# Function to analyze text using a BERT model
def analyze_text_with_bert(text_chunks):
    # Load a BART model pipeline for summarization
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summaries = []

    # Summarize each chunk of text with a specified max_length
    for chunk in text_chunks:
        # Adjust max_length to be roughly half of the chunk length or a fixed value
        max_length = min(len(chunk.split()), 88)  # or any other logic to set max_length
        summary = summarizer(chunk, max_length=max_length)
        summaries.append(summary[0]['summary_text'])

    # Combine all summaries into one string
    combined_summary = ' '.join(summaries)
    return combined_summary

# Main execution block
if __name__ == "__main__":
    pdf_path = 'Test_Rep2.pdf'  # Path to the PDF file

    # Extract text from the PDF
    extracted_text = extract_text_from_pdf(pdf_path)

    # Chunk the text for better processing
    text_chunks = chunk_text(extracted_text, chunk_size=800)  # Adjust chunk size based on your observation

    # Analyze text using BERT
    insights = analyze_text_with_bert(text_chunks)

    # Print the generated insights
    print("Generated Insights:")
    print(insights)